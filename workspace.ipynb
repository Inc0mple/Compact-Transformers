{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "from torch.nn.parallel import DistributedDataParallel as NativeDDP\n",
    "\n",
    "from timm.data import create_dataset, create_loader, resolve_data_config, Mixup, FastCollateMixup, AugMixDataset\n",
    "from timm.models import create_model, safe_model_name, resume_checkpoint, load_checkpoint, model_parameters\n",
    "from timm.models.layers import convert_splitbn_model\n",
    "from timm.utils import *\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.utils import ApexScaler, NativeScaler\n",
    "\n",
    "from src import *\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    from apex.parallel import DistributedDataParallel as ApexDDP\n",
    "    from apex.parallel import convert_syncbn_model\n",
    "\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    has_apex = False\n",
    "print(\"hello\")\n",
    "\n",
    "# SAVE_PATH = \"data/cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organized images based on their labels!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Path to your existing 'train' folder and the CSV file\n",
    "base_path = './data/cifar10/'  # Adjust this if your path is different\n",
    "train_folder_path = os.path.join(base_path, 'train')\n",
    "train_labels_path = os.path.join(base_path, 'trainLabels.csv')\n",
    "\n",
    "# Read the CSV file\n",
    "labels_df = pd.read_csv(train_labels_path)\n",
    "\n",
    "# Create new main training directory\n",
    "new_train_path = os.path.join(base_path, 'new_train')\n",
    "os.makedirs(new_train_path, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame rows and move images to their respective label folders\n",
    "for index, row in labels_df.iterrows():\n",
    "    image_name = f\"{row['id']}.png\"\n",
    "    label = row['label']\n",
    "    \n",
    "    # Create directory for the label if it doesn't exist\n",
    "    label_dir = os.path.join(new_train_path, label)\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    \n",
    "    # Source and destination paths\n",
    "    src_path = os.path.join(train_folder_path, image_name)\n",
    "    dst_path = os.path.join(label_dir, image_name)\n",
    "    \n",
    "    # Move the image to its label directory\n",
    "    shutil.move(src_path, dst_path)\n",
    "\n",
    "print(\"Organized images based on their labels!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data3/train/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data3/train/cifar-10-python.tar.gz to data3/train\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data3/validation/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data3/validation/cifar-10-python.tar.gz to data3/validation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data3/validation\n",
       "    Split: Test"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.CIFAR10(root=\"data3/train\", train=True, download=True)\n",
    "torchvision.datasets.CIFAR10(root=\"data3/validation\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def unpickle(file):\n",
    "    \"\"\"Load byte data from file\"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "def save_images_from_batch(data, labels, label_names, root_dir):\n",
    "    \"\"\"Save images contained in the CIFAR-10 batch to individual .png files\"\"\"\n",
    "    for idx, img_data in enumerate(data):\n",
    "        label = labels[idx]\n",
    "        label_name = label_names[label]\n",
    "        folder_path = os.path.join(root_dir, label_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        # Reshape image data and convert to RGB format\n",
    "        R = img_data[0:1024].reshape(32, 32)\n",
    "        G = img_data[1024:2048].reshape(32, 32)\n",
    "        B = img_data[2048:].reshape(32, 32)\n",
    "        img = np.dstack((R, G, B))\n",
    "        \n",
    "        img_path = os.path.join(folder_path, f\"{idx}.png\")\n",
    "        im = Image.fromarray(img)\n",
    "        im.save(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load label names from batches.meta\n",
    "metadata = unpickle('data3/train/cifar-10-batches-py/batches.meta')\n",
    "label_names = metadata['label_names']\n",
    "\n",
    "# Process training batches data_batch_1 to data_batch_5\n",
    "for i in range(1, 6):\n",
    "    batch_file = f'data3/train/cifar-10-batches-py/data_batch_{i}'\n",
    "    batch_data = unpickle(batch_file)\n",
    "    save_images_from_batch(batch_data['data'], batch_data['labels'], label_names, 'train')\n",
    "\n",
    "# Process test batch\n",
    "test_batch_data = unpickle('data3/train/cifar-10-batches-py/test_batch')\n",
    "save_images_from_batch(test_batch_data['data'], test_batch_data['labels'], label_names, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Cifar-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def unpickle(file):\n",
    "    \"\"\"Load byte data from file\"\"\"\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "def save_images_from_batch(data, labels, label_names, root_dir):\n",
    "    \"\"\"Save images contained in the CIFAR-10 batch to individual .png files\"\"\"\n",
    "    for idx, img_data in enumerate(data):\n",
    "        label = labels[idx]\n",
    "        label_name = label_names[label]\n",
    "        folder_path = os.path.join(root_dir, label_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        # Reshape image data and convert to RGB format\n",
    "        R = img_data[0:1024].reshape(32, 32)\n",
    "        G = img_data[1024:2048].reshape(32, 32)\n",
    "        B = img_data[2048:].reshape(32, 32)\n",
    "        img = np.dstack((R, G, B))\n",
    "        \n",
    "        img_path = os.path.join(folder_path, f\"{idx}.png\")\n",
    "        im = Image.fromarray(img)\n",
    "        im.save(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = unpickle('data/cifar-100-python.tar/cifar-100-python/meta')\n",
    "fine_label_names = metadata['fine_label_names']\n",
    "train_data = unpickle('data/cifar-100-python.tar/cifar-100-python/train')\n",
    "save_images_from_batch(train_data['data'], train_data['fine_labels'], fine_label_names, 'train')\n",
    "test_data = unpickle('data/cifar-100-python.tar/cifar-100-python/test')\n",
    "save_images_from_batch(test_data['data'], test_data['fine_labels'], fine_label_names, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fine_label_names', 'coarse_label_names'])\n"
     ]
    }
   ],
   "source": [
    "print(metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metadata[\"coarse_label_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metadata[\"fine_label_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = unpickle('data/cifar-100-python.tar/cifar-100-python/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filenames', 'batch_label', 'fine_labels', 'coarse_labels', 'data'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = unpickle('data/cifar-100-python.tar/cifar-100-python/test')\n",
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ulip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
